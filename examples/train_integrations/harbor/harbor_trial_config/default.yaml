# @package harbor_trial_config
#
# Default Harbor Configuration for SkyRL training
#
# This config is passed directly to Harbor's TrialConfig. You can use any
# Harbor-supported options here. SkyRL only injects a few runtime values:
#   - task.path (from the prompt)
#   - agent.model_name (constructed from served_model_name)
#   - agent.kwargs.api_base (from SkyRL's HTTP endpoint)
#   - agent.kwargs.session_id (generated per-trial)
#
# See Harbor documentation for all available options:
# https://harborframework.com/docs
# And a full list of fields that TrialConfig supports:
# https://github.com/laude-institute/harbor/blob/5921cc40e8f6b5b0df0a3dc6354e0e679447eb54/src/harbor/models/trial/config.py

# Harbor TrialConfig fields below
# --------------------------------

# Where to store trial logging outputs
trials_dir: ~/trials

# Trial timeout multiplier (applied to default timeouts)
timeout_multiplier: 1.0

# Agent configuration
# See: harbor.models.trial.config.AgentConfig
agent:
  # Agent name. Options: terminus-2, terminus-1, terminus, oracle, claude-code,
  # aider, swe-agent, openhands, etc.
  name: terminus-2

  # The time given a single Trial to run. If fails, the Trial raises an AgentTimeoutError.
  override_timeout_sec: 1200

  # Agent-specific settings passed to the agent constructor
  kwargs:
    # Maximum number of agent episodes/iterations
    max_turns: 32

    # Suppress Harbor override warnings for max_turns
    suppress_max_turns_warning: true

    # Whether to enable context summarization when approaching token limits
    enable_summarize: false

    # Store all messages in the trial output (required for SkyRL training)
    store_all_messages: true

    # The only sampling param that directly gets passed to Terminus
    temperature: 1.0

    # Model info for token budgeting
    # NOTE: max_input_tokens should match +generator.engine_init_kwargs.max_model_len
    model_info:
      max_input_tokens: 32768
      max_output_tokens: 32768
      # Set costs to 0 for local training
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

    # Additional kwargs passed through to LiteLLM's acompletion() call.
    # These are forwarded via Terminus2 -> LiteLLM -> litellm.acompletion(**kwargs).
    # Any parameter that litellm.acompletion() accepts can be set here, including:
    #   - timeout: LLM request timeout in seconds (default: litellm's 600s)
    #   - max_retries: Number of retries at the OpenAI SDK level (default: 2)
    #   - top_p, top_k, frequency_penalty, presence_penalty, etc.
    llm_kwargs:
      # LLM request timeout in seconds. Higher values prevent premature retries.
      timeout: 900
      # Set to 0 to disable OpenAI SDK retries from LiteLLM.
      max_retries: 0
      # Sampling params that directly get passed to LiteLLM's acompletion() call.
      top_p: 1.0
      top_k: -1
      min_p: 0.0

# Environment configuration
# See: harbor.models.trial.config.EnvironmentConfig
environment:
  # Environment type. Options: daytona, docker, e2b, modal, runloop, gke
  type: daytona

  # Resource overrides (optional)
  override_cpus: 1
  override_memory_mb: 1024
  override_storage_mb: 1024
  suppress_override_warnings: true

  # Environment-specific settings
  kwargs:
    # sandbox_timeout_secs: 1800 # Modal-specific. Timeout of the sandbox after which it is automatically stopped.
    auto_stop_interval_mins: 30 # Daytona-specific. Minutes of inactivity before the sandbox is automatically stopped.


# Verifier configuration
# See: harbor.models.trial.config.VerifierConfig
verifier:
  # Set to true to skip verification (useful for debugging)
  disable: false

  # Override the verifier timeout (seconds)
  # override_timeout_sec: 300
